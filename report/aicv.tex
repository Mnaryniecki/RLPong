\documentclass{pwrarticle}

% --- Language and Encoding ---
\usepackage[english]{babel}
\usepackage{amsopn} % English language support

% --- Document Metadata (PWr Specific) ---
\title{Applying Reinforcement Learning to the Classic Game Pong\\[0.5em]\large Project Definition, Midterm and Final Report}\author{Mateusz Naryniecki}
\studentID{278154}
\faculty{Faculty of Electronics, Photonics and Microsystems}
\course{Artificial Inteligence and Computer Vision}
\date{\today}

% --- Logo Path ---
\logopath{logo PWr kolor poziom ang.png} % The class looks inside the 'logotype/' folder

\begin{document}

\maketitle
\tableofcontents
\newpage

	\section{Project name}
	RLPong [Name WIP] \href{https://github.com/Mnaryniecki/RLPong}{GitHub Repo}

	\section{Scope and short description of project}
	I am going to apply reinforcement learning to the game Pong where the Player (in this case neural network AI) is put up against a typical algorithm that tracks the ball without anything else.

	\section{Case studies}
    Inspiration\\
    \url{https://www.youtube.com/watch?v=DmQ4Dqxs0HI}\\[0.3em]
    The approach I'm going to use\\
    \url{https://www.youtube.com/watch?v=vXtfdGphr3c&t=13s}\\[0.3em]
    Article on Github by Andrej Karpathy\\
    \url{https://karpathy.github.io/2016/05/31/rl}

	\section{How you want to solve the problem?}

The goal is to train an AI agent to play Pong using reinforcement learning. Instead of raw images, the agent will use 8 numerical values as input: the position and velocity of both paddles and the ball. This makes the problem simpler and faster to train.

\subsection{Approach}
First, I will build a simple version of Pong where these 8 values are available each frame. The AI will choose one of three actions: move up, move down, or stay still. The goal is to learn which actions help it win more often.

\subsection{Neural network}
A small neural network will be used to decide what to do. It will take the 8 input values, process them through a few layers of neurons, and output the probability of each action. The network will be trained to pick actions that lead to higher scores over time.

\subsection{Pretraining and learning}
At the beginning, I plan to train the network in a supervised way by copying a simple rule-based opponent that tracks the ball. This will give the AI some basic skills before reinforcement learning starts. Then I will use a standard reinforcement learning algorithm, such as a policy gradient method, to let the AI improve through trial and error.

\subsection{Rewards}
The agent will get a reward of +1 for scoring a point and -1 for losing a point. These rewards will guide the learning process so the AI learns to keep the ball in play and eventually beat the opponent.

\subsection{Evaluation}
Progress will be measured by how often the AI wins against the basic opponent and how its average reward increases over time. I will also record and plot training results to show improvements during learning.

	\section{Short plan with some big milestones together with dates}

\begin{itemize}
    \item \textbf{31.10 – 16.11.2025 (Design)} — Set up the Pong environment and define the 8 input values: the positions and velocities of both paddles and the ball. Make sure the basic game loop and the rule-based opponent work correctly.

    \item \textbf{17.11 – 30.11.2025 (Neural network setup)} — Build a small neural network (MLP) that takes the 8 inputs and outputs the three possible actions (up, down and optionally stay). Test the network to ensure it produces reasonable outputs.

    \item \textbf{01.12 – 14.12.2025 (Supervised pretraining)} — Collect gameplay data from the rule-based opponent and train the network to imitate it. Verify that the AI can follow the ball and react properly.

    \item \textbf{15.12.2025 – 04.01.2026 (Reinforcement learning)} — Train the AI using reinforcement learning to improve by playing against the opponent and learning from rewards.

    \item \textbf{05.01 – 11.01.2026 (Evaluation and fine-tuning)} — Measure the AI’s performance, record win rates, adjust hyperparameters if needed, and save gameplay for analysis.

    \item \textbf{12.01 – 18.01.2026 (Final report)} — Write and finalise the documentation, including explanations of methods, results, and ideas for future improvements, ensuring it is ready for submission by the deadline.
\end{itemize}

\section{Agreement}
	(proposal) I, Mateusz Naryniecki agree to deliver the project within defined timeline, within defined scope. Mateusz Cholewiński, PhD is confirming to grade it in an appropriate way, taking following document as a base. All changes, especially in timeline, scope, has to be agreed by both parties.

% =================================================================
% MIDTERM REPORT PART
% =================================================================

\section{Midterm report: implementation status}

This section describes what has been implemented so far, how it matches the original plan, and what remains to be done.

\subsection{Environment and game dynamics}

A custom Pong environment \texttt{PongEnv} has been implemented in Python. The game is played on a fixed-size window of \texttt{800x600} pixels with a square ball of size \texttt{20} and paddles of height \texttt{100} and width \texttt{20}. The ball speed is fixed at a constant magnitude, but its direction is initialised with a random angle so that it always moves either towards the left or towards the right side of the screen.

On reset, the environment:
\begin{itemize}
    \item Resets both scores to zero.
    \item Places both paddles in the vertical centre.
    \item Resets the ball to the centre with a random initial direction biased horizontally.
\end{itemize}

Collisions with the top and bottom walls are handled by inverting the vertical component of the ball velocity. Paddle collisions are implemented such that the outgoing angle depends on where the ball hits the paddle (more extreme angles near the edges, more horizontal in the centre). This produces more interesting and less predictable trajectories.

A game terminates if either player reaches a fixed number of points (e.g. 5) or when a maximum number of frames is exceeded, in which case the episode ends with a ``timeout'' result. The environment returns both a scalar reward and an \texttt{info} dictionary that stores who won, the scores, and the total number of frames.

\subsection{State representation and action space}

As planned, the state is represented by 8 normalised numerical values:
\begin{itemize}
    \item Ball horizontal position: $x_{\text{ball}} / \text{WIDTH}$.
    \item Ball vertical position: $y_{\text{ball}} / \text{HEIGHT}$.
    \item Ball horizontal velocity: $v_{x} / \text{speed}$.
    \item Ball vertical velocity: $v_{y} / \text{speed}$.
    \item Right paddle vertical position: $y_{\text{right}} / \text{HEIGHT}$.
    \item Right paddle movement direction (last action): $\{-1, 0, +1\}$.
    \item Left paddle vertical position: $y_{\text{left}} / \text{HEIGHT}$.
    \item Left paddle movement direction: $\{-1, 0, +1\}$.
\end{itemize}

This matches the initial design decision to avoid raw pixel inputs, simplifying the learning problem and speeding up training.

The action space for the learning agent (right paddle) consists of three discrete moves:
\begin{itemize}
    \item \textbf{0}: move up,
    \item \textbf{1}: stay still,
    \item \textbf{2}: move down.
\end{itemize}

The left paddle is controlled by a simple rule-based policy that tries to track the ball vertically with a fixed speed. This opponent acts as a baseline and as a teacher for pretraining.

\subsection{Reward structure}

The reward function follows the original plan:
\begin{itemize}
    \item The right (learning) agent receives $+1$ for scoring a point.
    \item It receives $-1$ for losing a point.
    \item Intermediate frames give $0$ reward.
\end{itemize}

I might change the reward system to penalize and reward later actions as they are probably the most impactful on the victory/loss

\subsection{Neural network policy}

The policy network, \texttt{PongNet}, is a small multilayer perceptron (MLP) with the following structure:
\begin{itemize}
    \item Input layer: dimension 8 (state vector).
    \item Two hidden layers with 64 units each and ReLU activations.
    \item Output layer: 3 units corresponding to the three possible actions.
\end{itemize}

The network outputs raw logits. During action selection, either a greedy policy (argmax over logits) or a stochastic policy (sampling from a softmax distribution with optional temperature) can be used. A wrapper class \texttt{PongAgent} handles device selection (CPU/GPU), model loading, and action selection in both single-state and batched mode.

\section{Midterm report: supervised pretraining}

\subsection{Synthetic teacher data}

Instead of recording data from live games, synthetic supervision data is generated directly from the state description. Each sample consists of:
\begin{itemize}
    \item A random ball position $(x, y)$, uniformly sampled in $[0, 1] \times [0, 1]$.
    \item A random ball velocity $(v_x, v_y)$, where each component is sampled from $[-1, 1]$.
    \item Random paddle positions $y_{\text{left}}, y_{\text{right}}$ in $[0, 1]$.
    \item Paddle directions initially set to $0$.
\end{itemize}

Given this state, a ``teacher'' rule determines the desired action for the right paddle mimicking the opponent in order to have enough "understanding" to continue learning :
\begin{itemize}
    \item If the ball is clearly above the right paddle (difference $< -\text{margin}$), the correct action is \texttt{up}.
    \item If the ball is clearly below the right paddle (difference $> \text{margin}$), the correct action is \texttt{down}.
    \item If the ball is already aligned (within the margin), the action is \texttt{stay}.
\end{itemize}

This generates a dataset of $(\text{state}, \text{action})$ pairs that approximate how a simple tracking algorithm would behave, but in a much more diversified set of situations than just replaying a few games.

\subsection{Supervised training procedure}

A separate script performs supervised training of \texttt{PongNet} on the synthetic dataset. The key components are:
\begin{itemize}
    \item Loss function: cross-entropy between the predicted logits and the teacher action labels.
    \item Optimiser: Adam with a learning rate of $10^{-3}$.
    \item Number of samples: e.g.\ 20\,000 synthetic states.
    \item Training loop: multiple epochs over all data, printing the loss for monitoring.
\end{itemize}

After training converges, the model parameters are saved to a file (e.g.\ \texttt{pong\_pretrained\_teacher.pth}). The \texttt{PongAgent} automatically attempts to load these weights on initialisation and switches to evaluation mode for inference.

At this midterm stage, the supervised pretraining phase is implemented and ready to be combined with reinforcement learning.

\section{Midterm report: evaluation and visualisation}

\subsection{Vectorised evaluation}

To efficiently measure the performance of the current agent, a vectorised evaluation script is provided. It creates multiple independent \texttt{PongEnv} instances, runs them in parallel using batched action selection (\texttt{act\_batch}), and counts:
\begin{itemize}
    \item Number of games won by the right (learning) agent.
    \item Number of games won by the left (rule-based) opponent.
    \item Number of timeouts (no player reaches the winning score before the frame limit).
\end{itemize}

By increasing the number of environments and episodes, this script allows for statistically meaningful estimates of win rates and for timing measurements (e.g.\ average time per environment per game). These metrics will later be used to compare:
\begin{itemize}
    \item The initial random policy.
    \item The supervised-pretrained policy.
    \item The final reinforcement learning policy.
\end{itemize}

\subsection{Graphical interface}

For qualitative inspection and debugging, a simple visualisation using \texttt{pygame} has been implemented. It:
\begin{itemize}
    \item Opens a window of size \texttt{WIDTH} by \texttt{HEIGHT}.
    \item Draws the left and right paddles and the ball each frame.
    \item Renders the current score of both players.
    \item Lets the right paddle be controlled by the current \texttt{PongAgent} policy.
\end{itemize}

This visualisation is useful for checking whether the pre-trained policy behaves as expected: tracking the ball, reacting to bounces, and recovering after losing points.

\section{Midterm report: comparison with original plan}

Here is how the current status relates to the milestones defined in the project proposal:

\begin{itemize}
    \item \textbf{31.10 – 16.11.2025 (Design):} The Pong environment, state representation, reward function, and rule-based opponent have been fully implemented and tested. The environment supports both single-episode runs and batched evaluation.

    \item \textbf{17.11 – 30.11.2025 (Neural network setup):} The MLP architecture has been implemented and integrated with an agent wrapper. Action selection supports both greedy and stochastic modes, and the network can be run on CPU or GPU.

    \item \textbf{01.12 – 14.12.2025 (Supervised pretraining):} The synthetic teacher data generation and supervised training script are implemented. The network can be pretrained to imitate a ball-tracking policy and saved to disk for later use in reinforcement learning.

    \item \textbf{15.12.2025 – 04.01.2026 (Reinforcement learning):} Reinforcement learning (e.g.\ policy gradients or another on-policy method) is the next step. The environment and pretrained policy are ready, but the RL training loop and logging have not yet been implemented at this midterm stage.

    \item \textbf{05.01 – 11.01.2026 (Evaluation and fine-tuning):} The evaluation infrastructure is already partially in place via the parallel evaluation script. It will be extended to log learning curves (win rate, average reward) during RL training.

    \item \textbf{12.01 – 18.01.2026 (Final report):} The current document serves as a combined project definition and midterm report. A final extended version will include detailed experimental results, plots, and a discussion of limitations and possible improvements.
\end{itemize}

\section{Next steps}

The main next steps towards the final project are:

\begin{itemize}
    \item Implement a reinforcement learning algorithm (e.g.\ REINFORCE or an actor-critic variant) on top of the existing environment and pretrained policy.
    \item Add logging of episode returns, win rates, and possibly entropy or KL-divergence to monitor exploration.
    \item Run experiments comparing:
    \begin{itemize}
        \item Training from scratch vs.\ starting from the supervised teacher.
        \item Different reward shaping or curriculum variations (e.g.\ starting with slower ball speed).
    \end{itemize}
    \item Produce plots and visualisations of learning progress.
    \item Integrate selected games into the report via screenshots or short descriptions of interesting behaviours (e.g.\ long rallies, exploiting paddle angles).
\end{itemize}

These steps will complete the transition from a working Pong environment and supervised policy to a fully trained reinforcement learning agent, and will provide the material for the final report.


% =================================================================
% PART 3: FINAL REPORT
% =================================================================
\section{Final Report: Reinforcement Learning Implementation}

    Following the midterm milestones, the focus shifted to implementing the Reinforcement Learning (RL) loop, automating the experimental workflow, and establishing a robust evaluation pipeline.

    \subsection{Algorithm and Training Loop}
    The REINFORCE algorithm (Monte Carlo Policy Gradient) was selected for training. The implementation in \texttt{train\_rl.py} utilizes:
    \begin{itemize}
        \item \textbf{Parallel Environments:} To stabilize gradients and speed up data collection, 256 independent \texttt{PongEnv} instances run in parallel. This is crucial for collecting diverse trajectories efficiently.
        \item \textbf{Hyperparameters:}
        \begin{itemize}
            \item Learning Rate: $1 \times 10^{-4}$ (Adam optimizer).
            \item Discount Factor ($\gamma$): 0.99.
            \item Entropy Coefficient: 0.01 (to encourage exploration).
        \end{itemize}
        \item \textbf{Batch Updates:} Gradients are computed and applied after collecting trajectories from all parallel environments.
        \item \textbf{Return Normalization:} Discounted returns are normalized across the batch to reduce variance and improve training stability.
    \end{itemize}

    To ensure the agent learns a robust policy, the training process includes a ``waterfall'' loading mechanism: it attempts to resume from a previous RL checkpoint, falls back to the supervised ``teacher'' model, and finally defaults to random initialization if no weights are found.

    \subsection{Experiment Orchestration}
    A dedicated script, \texttt{run\_experiment.py}, was developed to automate the full experimental lifecycle. It performs the following steps sequentially:
    \begin{enumerate}
        \item \textbf{Cleanup:} Removes artifacts from previous runs (unless resuming) to ensure a clean state.
        \item \textbf{Baseline Evaluation:} Evaluates a random agent (``Scratch'') and the supervised agent (``Teacher'') to establish performance benchmarks before RL begins.
        \item \textbf{Incremental Training:} Runs the training loop in blocks of 16 updates (approx 4096 games per block), pausing to evaluate the current model against the rule-based opponent.
        \item \textbf{Logging:} Results (Win Rate, Average Reward) are logged to CSV files (\texttt{results\_scratch\_stochastic.csv} and \texttt{results\_teacher\_stochastic.csv}) after every evaluation step.
    \end{enumerate}

    \subsection{Model Selection and Evaluation}
    Evaluation is performed using \texttt{eval.py}, which tests the agent in 128 parallel environments for a fixed number of episodes (16 per evaluation step).
    
    Crucially, the training loop tracks the \textbf{Average Reward} to identify the best-performing model. The weights corresponding to the highest average reward are saved separately as \texttt{[prefix]\_best.pth}. This ensures that the final deployed agent represents the peak of training performance rather than the latest (potentially unstable) iteration.

    \subsection{Results Visualization}
    A plotting utility, \texttt{plot\_results.py}, was implemented to visualize the training progress. It reads the generated CSV logs and produces a dual-axis plot showing:
    \begin{itemize}
        \item \textbf{Win Rate (\%):} The percentage of games won against the rule-based opponent.
        \item \textbf{Average Reward:} The mean reward per game, providing a more granular measure of performance than binary win/loss.
    \end{itemize}

    \subsection{Codebase Refactoring}
    To support these advanced features, the codebase underwent significant refactoring:
    \begin{itemize}
        \item \textbf{Configuration:} All game constants (dimensions, speeds, rewards) were centralized in \texttt{config.py} to ensure consistency across simulation, training, and visualization.
        \item \textbf{Robustness:} The environment includes checks (e.g., \texttt{is\_catchable}) to ensure fair initialization, preventing the agent from being penalized for physically impossible scenarios.
        \item \textbf{Visualization:} The \texttt{visual\_game.py} script was updated to load the best model by default and supports command-line arguments for flexibility.
    \end{itemize}


    \subsection{Updated Experimental Methodology}

    \subsubsection{Stochastic Evaluation}
     To provide a more rigorous assessment of the agent's capabilities, the evaluation protocol in \texttt{eval.py} was updated to use a \textbf{stochastic policy} ($\texttt{stochastic=True}$). Unlike greedy evaluation, which deterministically selects the action with the highest probability, stochastic evaluation samples from the action distribution output by the policy network. This approach ensures that the reported Win Rate and Average Reward metrics reflect the agent's general robustness and ability to handle variability, rather than its ability to exploit deterministic trajectories against the fixed rule-based opponent.

    \subsubsection{Automated Comparison: Scratch vs. Teacher}
     The experiment orchestration script, \texttt{run\_experiment.py}, was refactored to conduct a direct comparative analysis. It now automates the execution of two sequential training regimes:
    \begin{enumerate}
         \item \textbf{Scratch Stochastic:} Training an agent initialized with random weights to establish a baseline learning curve.
         \item \textbf{Teacher Stochastic:} Fine-tuning an agent pre-trained on the rule-based opponent's behavior to evaluate the benefits of transfer learning.
    \end{enumerate}
     Results from these experiments are logged to separate CSV files, enabling side-by-side visualization of convergence speed and final performance.

    \subsubsection{Robustness and Recovery}
     To support long-running experimental campaigns, a resume mechanism was implemented in the training loop. The system checks for existing result logs and model checkpoints before starting. If an experiment is interrupted (e.g., due to a system restart), it automatically resumes training from the last recorded update step, preserving previously collected data and computational resources.

    \subsection{Results and Discussion}
    
    \begin{figure}[H]
        \centering
        \includegraphics[width=1.0\textwidth]{comparison_plot_stochastic.png}
        \caption{Comparison of learning curves between Scratch (Blue) and Pre-trained Teacher (Orange) using Stochastic Evaluation. The Teacher model starts with a higher win rate and converges significantly faster to a stable 99\% win rate.}
        \label{fig:results_stochastic}
    \end{figure}

    As shown in Figure \ref{fig:results_stochastic}, the agent initialized with "Teacher" weights demonstrated significantly faster convergence compared to the "Scratch" agent. The pre-trained model began with a non-zero win rate and quickly optimized its policy to exploit the opponent's weaknesses. In contrast, the scratch model required approximately 200 updates (roughly 50,000 games) to simply reach positive average rewards, indicating the difficulty of discovering the ball-tracking strategy from random actions.
    
    \begin{figure}[H]
        \centering
        \includegraphics[width=1.0\textwidth]{comparison_plot_stochastic_vs_greedy.png}
        \caption{Performance gap between Greedy (Orange and Blue) and Stochastic (Purple and Green) evaluation policies. The stochastic metrics provide a more conservative and realistic estimate of the agent's true capability.}
        \label{fig:results_greedy_vs_stochastic}
    \end{figure}

    Figure \ref{fig:results_greedy_vs_stochastic} highlights a critical observation regarding the agent's stability. Surprisingly, the model performed better overall in \textbf{stochastic mode} compared to \textbf{greedy mode}. The greedy policy (taking the argmax of logits) proved to be more volatile: if the agent made a single suboptimal decision, the deterministic nature of the policy could cause it to get stuck in a failure loop or commit to a rigid trajectory that missed the ball.

    In contrast, stochastic evaluation allowed the model to sample actions based on probability, effectively "smoothing out" jittery decision-making. This randomness served as a recovery mechanism, preventing the agent from becoming locked into a bad decision and allowing it to adjust its trajectory dynamically. Consequently, stochastic evaluation provided a more stable and higher-performing metric, confirming that a slight degree of entropy is beneficial for robustness in continuous-time tracking tasks.

\newpage
\section{Summary}

This project successfully developed and evaluated a reinforcement learning agent for the classic game of Pong. The primary objective was to train a neural network to master the game using a state-based approach, bypassing the complexity of raw pixel inputs. The agent's "brain" is a multi-layer perceptron that processes an 8-dimensional state vector---containing the normalized positions and velocities of the ball and both paddles---to decide between moving up, down, or staying still.

The core of the training process utilized the \textbf{REINFORCE} (Monte Carlo Policy Gradient) algorithm. To ensure stable and efficient learning, the implementation was enhanced with several key techniques:
\begin{itemize}
    \item \textbf{Parallel Environments:} Training data was collected from 256 simultaneous game instances, providing a diverse batch of experiences for each policy update.
    \item \textbf{Return Normalization:} The cumulative rewards were normalized within each batch, a crucial step that acts as a baseline to reduce gradient variance.
    \item \textbf{Entropy Regularization:} A small entropy bonus was added to the loss function to encourage exploration and prevent the agent from prematurely converging to a suboptimal deterministic policy.
\end{itemize}

A significant aspect of this project was the rigorous, automated experimental framework designed to compare two fundamental training strategies:
\begin{enumerate}
    \item \textbf{Training from Scratch:} A baseline approach where the agent learns with no prior knowledge.
    \item \textbf{Training from a Teacher:} A transfer learning approach where the agent is first pre-trained via supervised learning to imitate a simple, rule-based opponent. It is then fine-tuned using reinforcement learning.
\end{enumerate}

The entire pipeline, from environment simulation and agent training to vectorized evaluation and results visualization, was automated. The system was designed not only to produce a capable Pong-playing agent but also to serve as a robust platform for conducting comparative machine learning experiments. The final evaluation, based on stochastic action selection, ensures that the reported metrics reflect the agent's general capability rather than its performance on deterministic trajectories.


\end{document}